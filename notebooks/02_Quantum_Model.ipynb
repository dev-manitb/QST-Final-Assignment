{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3d5a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 samples...\n",
      "✅ Data Generation Complete!\n",
      "   Inputs Shape: (10000, 6) (6 probabilities per sample)\n",
      "   Labels Shape: (10000, 2, 2) (2x2 density matrix per sample)\n",
      "   Saved to: qst_data_ml\\dataset.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import unitary_group\n",
    "import os\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "DATASET_SIZE = 10000  # Number of examples to generate\n",
    "N_SHOTS = 1024        # Shots per basis (Z, X, Y)\n",
    "SAVE_DIR = \"qst_data_ml\"\n",
    "\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "# --- 2. Helper: Generate Random Density Matrices ---\n",
    "def generate_random_rho(dim=2):\n",
    "    \"\"\"\n",
    "    Generates a random valid density matrix using the Ginibre ensemble.\n",
    "    This covers both pure and mixed states.\n",
    "    \"\"\"\n",
    "    # A is a random complex matrix\n",
    "    A = np.random.randn(dim, dim) + 1j * np.random.randn(dim, dim)\n",
    "    # rho = A * A_dagger\n",
    "    rho = A @ A.conj().T\n",
    "    # Normalize trace to 1\n",
    "    rho /= np.trace(rho)\n",
    "    return rho\n",
    "\n",
    "# --- 3. Helper: Measurement Operators (From Assignment 1) ---\n",
    "# We define the 6 projectors: Z0, Z1, X0, X1, Y0, Y1\n",
    "s0 = np.eye(2)\n",
    "sz = np.array([[1, 0], [0, -1]])\n",
    "sx = np.array([[0, 1], [1, 0]])\n",
    "sy = np.array([[0, -1j], [1j, 0]])\n",
    "\n",
    "# Projectors P = (I +/- sigma) / 2\n",
    "projectors = {\n",
    "    'Z0': (s0 + sz)/2, 'Z1': (s0 - sz)/2,\n",
    "    'X0': (s0 + sx)/2, 'X1': (s0 - sx)/2,\n",
    "    'Y0': (s0 + sy)/2, 'Y1': (s0 - sy)/2\n",
    "}\n",
    "proj_list = [projectors[k] for k in ['Z0', 'Z1', 'X0', 'X1', 'Y0', 'Y1']]\n",
    "\n",
    "# --- 4. Main Data Generation Loop ---\n",
    "print(f\"Generating {DATASET_SIZE} samples...\")\n",
    "\n",
    "inputs = []  # To store measurement frequencies (the \"features\")\n",
    "labels = []  # To store the flattened density matrix (the \"truth\")\n",
    "\n",
    "for i in range(DATASET_SIZE):\n",
    "    # A. Create a random state\n",
    "    rho = generate_random_rho()\n",
    "    \n",
    "    # B. Simulate Measurement (Born Rule + Noise)\n",
    "    freqs = []\n",
    "    for P in proj_list:\n",
    "        # Probability p = Tr(rho * P)\n",
    "        prob = np.real(np.trace(rho @ P))\n",
    "        # Ensure numerical stability\n",
    "        prob = np.clip(prob, 0, 1)\n",
    "        \n",
    "        # Simulate experimental noise (Binomial distribution)\n",
    "        # Counts = np.random.binomial(n=N_SHOTS, p=prob)\n",
    "        # We store the *Frequency* (Counts / Total Shots)\n",
    "        simulated_counts = np.random.binomial(N_SHOTS, prob)\n",
    "        freqs.append(simulated_counts / N_SHOTS)\n",
    "    \n",
    "    # C. Store Data\n",
    "    inputs.append(freqs)\n",
    "    # Store rho as a 4x1 vector of complex numbers, or separated real/imag\n",
    "    # For simplicity, we keep it as a 2x2 matrix in the numpy array\n",
    "    labels.append(rho)\n",
    "\n",
    "# --- 5. Save to Disk ---\n",
    "inputs = np.array(inputs, dtype=np.float32)  # Shape: (10000, 6)\n",
    "labels = np.array(labels, dtype=np.complex64) # Shape: (10000, 2, 2)\n",
    "\n",
    "save_path = os.path.join(SAVE_DIR, \"dataset.npz\")\n",
    "np.savez(save_path, X=inputs, y=labels)\n",
    "\n",
    "print(f\"✅ Data Generation Complete!\")\n",
    "print(f\"   Inputs Shape: {inputs.shape} (6 probabilities per sample)\")\n",
    "print(f\"   Labels Shape: {labels.shape} (2x2 density matrix per sample)\")\n",
    "print(f\"   Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759e5b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Created Successfully!\n",
      "Output Shape: torch.Size([5, 2, 2]) (Should be 5, 2, 2)\n",
      "Traces (should be all ~1.0): [0.99999917 0.9999995  0.9999994  0.9999996  0.9999987 ]\n",
      "Is Hermitian? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 1. The Physics-Informed Neural Network ---\n",
    "class QuantumStateReconstructor(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A simple 3-layer MLP (Hardware-friendly)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            # Output layer: We need 4 real numbers to define a 2x2 L matrix\n",
    "            # (2 Real Diagonals + 1 Complex Off-Diagonal = 4 params)\n",
    "            nn.Linear(hidden_dim, 4) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 6)\n",
    "        \n",
    "        # 1. Get raw outputs from the neural net\n",
    "        logits = self.network(x)\n",
    "        \n",
    "        # 2. Reshape into L matrix components\n",
    "        # We need L = [[l00, 0], [l10, l11]]\n",
    "        # l00, l11 are real. l10 is complex (real + imag).\n",
    "        \n",
    "        l00 = logits[:, 0]\n",
    "        l11 = logits[:, 1]\n",
    "        l10_real = logits[:, 2]\n",
    "        l10_imag = logits[:, 3]\n",
    "        \n",
    "        # 3. Construct the Lower Triangular Matrix L\n",
    "        batch_size = x.shape[0]\n",
    "        L = torch.zeros((batch_size, 2, 2), dtype=torch.complex64, device=x.device)\n",
    "        \n",
    "        # Fill the matrix elements\n",
    "        L[:, 0, 0] = l00.type(torch.complex64)\n",
    "        L[:, 1, 0] = torch.complex(l10_real, l10_imag)\n",
    "        L[:, 1, 1] = l11.type(torch.complex64)\n",
    "        \n",
    "        # 4. Compute rho = L @ L_dagger\n",
    "        # Batch Matrix Multiplication (bmm)\n",
    "        L_dagger = torch.transpose(L.conj(), 1, 2)\n",
    "        rho_unscaled = torch.bmm(L, L_dagger)\n",
    "        \n",
    "        # 5. Normalize (Trace = 1)\n",
    "        # Trace is the sum of diagonal elements (dim1=1, dim2=2)\n",
    "        trace = torch.diagonal(rho_unscaled, dim1=1, dim2=2).sum(dim=1)\n",
    "        \n",
    "        # Add epsilon to prevent divide-by-zero\n",
    "        trace = trace.view(-1, 1, 1) + 1e-8\n",
    "        \n",
    "        rho = rho_unscaled / trace\n",
    "        \n",
    "        return rho\n",
    "\n",
    "# --- 2. Verification Test ---\n",
    "# Let's verify the model outputs valid quantum states immediately.\n",
    "if __name__ == \"__main__\":\n",
    "    model = QuantumStateReconstructor()\n",
    "    \n",
    "    # Fake input: 5 examples of measurement data\n",
    "    dummy_input = torch.rand(5, 6) \n",
    "    \n",
    "    # Predict\n",
    "    predicted_rho = model(dummy_input)\n",
    "    \n",
    "    print(\"Model Architecture Created Successfully!\")\n",
    "    print(f\"Output Shape: {predicted_rho.shape} (Should be 5, 2, 2)\")\n",
    "    \n",
    "    # Check Trace Constraint\n",
    "    traces = torch.diagonal(predicted_rho, dim1=1, dim2=2).sum(dim=1)\n",
    "    print(f\"Traces (should be all ~1.0): {traces.real.detach().numpy()}\")\n",
    "    \n",
    "    # Check if Hermitian (rho == rho_dagger)\n",
    "    rho_dagger = torch.transpose(predicted_rho.conj(), 1, 2)\n",
    "    is_hermitian = torch.allclose(predicted_rho, rho_dagger, atol=1e-6)\n",
    "    print(f\"Is Hermitian? {is_hermitian}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c23d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Data Loaded. Training on 8000 samples, Testing on 2000.\n",
      "\n",
      "Starting Training for 50 epochs...\n",
      "Epoch 5/50 | Loss: 0.001052 | Avg Fidelity: 0.7924\n",
      "Epoch 10/50 | Loss: 0.000877 | Avg Fidelity: 0.7940\n",
      "Epoch 15/50 | Loss: 0.000813 | Avg Fidelity: 0.7949\n",
      "Epoch 20/50 | Loss: 0.000834 | Avg Fidelity: 0.7959\n",
      "Epoch 25/50 | Loss: 0.001048 | Avg Fidelity: 0.7948\n",
      "Epoch 30/50 | Loss: 0.000821 | Avg Fidelity: 0.7928\n",
      "Epoch 35/50 | Loss: 0.000760 | Avg Fidelity: 0.7950\n",
      "Epoch 40/50 | Loss: 0.000850 | Avg Fidelity: 0.7947\n",
      "Epoch 45/50 | Loss: 0.000831 | Avg Fidelity: 0.7961\n",
      "Epoch 50/50 | Loss: 0.000759 | Avg Fidelity: 0.7953\n",
      "\n",
      "✅ Training Complete! Model saved to outputs/model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading dataset...\")\n",
    "data = np.load(\"qst_data_ml/dataset.npz\")\n",
    "X_numpy = data['X']  # Shape: (10000, 6)\n",
    "y_numpy = data['y']  # Shape: (10000, 2, 2)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "# Inputs are Float32\n",
    "X_tensor = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "# Labels are Complex64\n",
    "y_tensor = torch.tensor(y_numpy, dtype=torch.complex64)\n",
    "\n",
    "# Split into Train (80%) and Test (20%)\n",
    "split_idx = int(0.8 * len(X_tensor))\n",
    "X_train, X_test = X_tensor[:split_idx], X_tensor[split_idx:]\n",
    "y_train, y_test = y_tensor[:split_idx], y_tensor[split_idx:]\n",
    "\n",
    "print(f\"Data Loaded. Training on {len(X_train)} samples, Testing on {len(X_test)}.\")\n",
    "\n",
    "# --- 2. Setup Training ---\n",
    "# Re-initialize the model\n",
    "model = QuantumStateReconstructor() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss Function: Mean Squared Error between the two density matrices\n",
    "# We calculate Norm(rho_pred - rho_true)^2\n",
    "def complex_mse_loss(pred, target):\n",
    "    diff = pred - target\n",
    "    # Frobenius norm squared\n",
    "    loss = torch.norm(diff) ** 2\n",
    "    return loss / len(pred) # Normalize by batch size\n",
    "\n",
    "# Helper for Fidelity (Metric)\n",
    "def calculate_batch_fidelity(pred, target):\n",
    "    # For 2x2, simpler fidelity formula or just use overlap for pure states\n",
    "    # But since these are mixed, we compute standard Fidelity approximation or exact\n",
    "    # F(rho, sigma) = (Tr(sqrt(sqrt(rho) sigma sqrt(rho))))^2\n",
    "    # For training speed, we'll assume pure states overlap approx: Tr(rho * sigma)\n",
    "    # (Note: This is exact for pure states, approximation for mixed)\n",
    "    \n",
    "    # Let's use a simpler proxy for monitoring: Real part of Trace(pred * target)\n",
    "    overlap = torch.matmul(pred, target)\n",
    "    fid = torch.diagonal(overlap, dim1=1, dim2=2).sum(dim=1).real\n",
    "    return fid.mean().item()\n",
    "\n",
    "# --- 3. Training Loop ---\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"\\nStarting Training for {EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # Mini-batch shuffling\n",
    "    indices = torch.randperm(len(X_train))\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(0, len(X_train), BATCH_SIZE):\n",
    "        batch_idx = indices[i : i + BATCH_SIZE]\n",
    "        batch_X = X_train[batch_idx]\n",
    "        batch_y = y_train[batch_idx]\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        \n",
    "        # Loss\n",
    "        loss = complex_mse_loss(predictions, batch_y)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation Step (every 5 epochs)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_preds = model(X_test)\n",
    "            val_loss = complex_mse_loss(test_preds, y_test)\n",
    "            avg_fid = calculate_batch_fidelity(test_preds, y_test)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {val_loss:.6f} | Avg Fidelity: {avg_fid:.4f}\")\n",
    "\n",
    "# --- 4. Save Model ---\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "save_path = \"outputs/model_weights.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"\\n✅ Training Complete! Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa4e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Model loaded. Starting evaluation...\n",
      "\n",
      "========================================\n",
      "   FINAL PERFORMANCE REPORT (Track 2)\n",
      "========================================\n",
      "1. Mean Fidelity:       0.7953\n",
      "2. Mean Trace Distance: 0.0179\n",
      "3. Inference Latency:   0.0211 ms/sample\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 1. Setup & Load Model ---\n",
    "# Detect device (CPU is fine for evaluation)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading test data...\")\n",
    "data = np.load(\"qst_data_ml/dataset.npz\")\n",
    "X_test = torch.tensor(data['X'][8000:], dtype=torch.float32).to(device) # Last 2000 samples\n",
    "y_test = torch.tensor(data['y'][8000:], dtype=torch.complex64).to(device)\n",
    "\n",
    "# Load Model\n",
    "model = QuantumStateReconstructor().to(device)\n",
    "model.load_state_dict(torch.load(\"outputs/model_weights.pth\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded. Starting evaluation...\")\n",
    "\n",
    "# --- 2. Calculate Metrics (Fidelity & Trace Distance) ---\n",
    "def compute_metrics(pred_rho, true_rho):\n",
    "    \"\"\"\n",
    "    Computes Fidelity and Trace Distance for a batch.\n",
    "    Uses PyTorch linear algebra.\n",
    "    \"\"\"\n",
    "    # 1. Trace Distance: T = 0.5 * Tr|rho - sigma| = 0.5 * sum(abs(eigenvalues(diff)))\n",
    "    diff = pred_rho - true_rho\n",
    "    # Eigenvalues of the difference matrix\n",
    "    eigvals = torch.linalg.eigvalsh(diff) \n",
    "    trace_dist = 0.5 * torch.sum(torch.abs(eigvals), dim=1)\n",
    "    \n",
    "    # 2. Fidelity: F = (Tr(sqrt(sqrt(rho) sigma sqrt(rho))))^2\n",
    "    # For mixed states, this is hard to vectorize perfectly in vanilla PyTorch.\n",
    "    # We will use the standard approximation for validation: Tr(rho * sigma) \n",
    "    # (Exact for pure states, lower bound for mixed).\n",
    "    # IF you need exact mixed state fidelity, we loop and use scipy/qutip (slower).\n",
    "    \n",
    "    # Let's stick to the overlap metric we used in training for consistency, \n",
    "    # but strictly it is 'State Overlap'.\n",
    "    overlap_matrix = torch.matmul(pred_rho, true_rho)\n",
    "    overlap = torch.diagonal(overlap_matrix, dim1=1, dim2=2).sum(dim=1).real\n",
    "    \n",
    "    # Clamp to [0,1] just in case\n",
    "    fidelity = torch.clamp(overlap, 0.0, 1.0)\n",
    "    \n",
    "    return fidelity, trace_dist\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Run Inference on all test data\n",
    "    start_time = time.time()\n",
    "    predictions = model(X_test)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate Latency\n",
    "    total_time = end_time - start_time\n",
    "    latency_per_sample = (total_time / len(X_test)) * 1000 # in ms\n",
    "    \n",
    "    # Calculate Physics Metrics\n",
    "    fidelities, trace_dists = compute_metrics(predictions, y_test)\n",
    "    \n",
    "    mean_fidelity = fidelities.mean().item()\n",
    "    mean_trace = trace_dists.mean().item()\n",
    "\n",
    "# --- 3. Final Report ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"   FINAL PERFORMANCE REPORT (Track 2)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"1. Mean Fidelity:       {mean_fidelity:.4f}\")\n",
    "print(f\"2. Mean Trace Distance: {mean_trace:.4f}\")\n",
    "print(f\"3. Inference Latency:   {latency_per_sample:.4f} ms/sample\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Save these numbers for your README!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
